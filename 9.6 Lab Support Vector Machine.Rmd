---
title: "9.6 Lab Support Vector Machine"
author: "WangYong"
date: "2022/2/27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
RNGkind(sample.kind = "Rounding")
```
## 9.6 Support Vector Machine

### Support Vector Classifier

- prepare data
```{r}
set.seed(1)

x= matrix(rnorm(20*2), ncol=2)
y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,]+1
plot(x, col=(3-y))
data = data.frame(x=x, y=as.factor(y))
```
See the svm with cost =10
```{r}
svmfit <-svm(y~., data=data, kernel='linear', cost=10,scale=F)
plot(svmfit, data)
```
```{r}
svmfit$index
summary(svmfit        )
```

See the svm with cost =0.1
```{r}
svmfit <-svm(y~., data=data, kernel='linear', cost=0.1,scale=F)
plot(svmfit, data)
```
-see the svmfit index & summary
```{r}
svmfit$index
summary(svmfit)
```
- tune the parameter of cost by tune function
coss =0.1 can get best model

```{r}
set.seed(1)
tune.out<- tune(svm,y~., data=data, kernel='linear',
                ranges=list(cost=c(0.001,0.01,0.1,1.5,10,100)))
summary(tune.out)
bestmode <-tune.out$best.model
summary(bestmode)

```

- check the test error with best model

```{r}
xtest <- matrix(rnorm(20*2),ncol=2)
ytest <- sample(c(-1,1),20, rep=T)

xtest[ytest==1,] =xtest[ytest==1,]+1
testdata = data.frame(x=xtest,y=as.factor(ytest))

ypred = predict(bestmode, testdata)

table(predict=ypred, truth=testdata$y)
```
 11+8 precict is right under best model. let's check the poor model with cost=0.01
 
```{r}
svmfit<- svm(y~., data=data, kernel='linear', cost=0.01)
ypred = predict(svmfit, testdata)
table(predict=ypred, truth=testdata$y)

```
it could be found only 18 is right, less the best model

- very large cost 
  - no observation is misclassified( best train results)
  - margin is very narrow( because ,no support vectors. maybe overfit.)

```{r}
x[y==1,]=x[y==1,]+0.5
plot(x, col=(y+5)/2, pch=19)
data <- data.frame(x=x, y=as.factor(y))
svmfit <- svm(y~., data=data, kernel='linear', cost=1e5)
summary(svmfit)
```

let's try small cost =1
```{r}
svmfit<- svm(y~., data=data, kernel='linear', cost=1)
summary(svmfit)
plot(svmfit, data)
```
#### 9.6.2 Support Vector Machine
Using non-linear kernel

```{r}
set.seed(1)
x=matrix(rnorm(200*2) , ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150 ,]=x[101:150,]-2
y=c(rep(1,150) ,rep(2,50))
data=data.frame(x=x,y=as.factor(y))
plot(x,col=y)
```
```{r}
train <- sample(200,100)
library(manipulate)
show_svm<- function(cost){
  svmfit<- svm(y~., data=data[train,],kernel='radial', gamma=1, cost=cost)
  plot(svmfit, data[train,])
}
manipulate(show_svm(cost),cost=slider(1,1e5,step = 1000))
#summary(svmfit)
```

We can see from the figure that, there are fair number of training error in fit.
if we increase the value of cost, the train error could be reduced but lead to
irregular boundary and seems to overfit.

- tune svm to find best lambda and cost
```{r}
set.seed(1)
tune.out <- tune(svm,y~., 
                 data=data[train,],
                 kernel='radial',
                 range=list(cost=c(0.1,1,10,100,1000),
                 gamma=c(0.5, 1,2,3,4)))

summary(tune.out)
table(true=data[-train,'y'], pred=predict(tune.out$best.model,newdata=data[-train,]))
```

