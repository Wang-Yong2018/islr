---
title: "8.3 Lab decision Tree"
author: "WangYong"
date: "2022/2/25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(tree)
RNGkind(sample.kind = 'Rounding') 
#attach(Carseats)
High <-factor(ifelse(Carseats$Sales<=8,'No','Yes'))

tmp_df <- data.frame(Carseats, High)
```

## 8.3 Lab Decision trees

### 8.3.1 Fitting Classification Trees
- summary
```{r}
tree.carseats <- tree(High~ .-Sales, data=tmp_df)
#as.numeric(tree.carseats$y)
summary(tree.carseats)
tree.carseats
```

- plot tree
```{r}
plot(tree.carseats)
text(tree.carseats,pretty=0)
```

- predict test
```{r}
set.seed(2)
train <- sample(1:nrow(Carseats),200)
Carseats.test <- Carseats[-train,]
High.test = High[-train]

tree.carseats <- tree(High~.-Sales, 
                      data=Carseats,
                      subset=train)
tree.pred <- predict(tree.carseats, Carseats.test,type='class')
table(tree.pred,High.test)
(86+57)/200 # the diag / rows of pred
```
- cross validation
```{r}
cv.carseats <- cv.tree(tree.carseats, FUN=prune.misclass)
names(cv.carseats)
cv.carseats
attr(cv.carseats,"class")
```
note, dev response to the cross validation error. with 9 termination node, the 
results is lowest. with 49. Next we will plot it.

- plot 
```{r}
par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev, type='b')
plot(cv.carseats$k,cv.carseats$dev, type='b')
```
- plot missclass prune 9
```{r}
prune.carseats =prune.misclass (tree.carseats ,best=9)
plot(prune.carseats )
text(prune.carseats ,pretty =0)
```
- check the prune result on test error
with prune, we get better test result. lower the test error.
```{r}
tree.pred <- predict(prune.carseats,Carseats.test,type='class')
table(tree.pred, High.test)
(94+60)/200
```

### 8.3.2 Fitting the Regression Tree
on Boston data
```{r}
library(MASS)
set.seed(1)
train <-sample(1:nrow(Boston),nrow(Boston)/2)
tree.boston <- tree(medv~., Boston, subset=train)
summary(tree.boston)
tree.boston
```

- plot boston tree
```{r}
plot(tree.boston)
text(tree.boston, pretty=0)
```
- cross validation(10 folds)
in this case, most complex tree is selected by cv
```{r}
cv.boston <- cv.tree(tree.boston,K=10)
plot(cv.boston$size, cv.boston$dev,type='b')
```
- prune tree
```{r}
prune.boston <- prune.tree(tree.boston, best=5)
plot(prune.boston)
text(prune.boston,pretty=0)
```
- predict with unprune tree
```{r}
yhat <- predict(tree.boston, newdata = Boston[-train,])
boston.test = Boston[-train,'medv']
plot(yhat, boston.test,col='blue')
abline(0,1)
mean((yhat-boston.test)^2)
```


### 8.3.3 Bagging and Random Forests
```{r}
library(randomForest)
RNGkind(sample.kind = 'Rounding') 
set.seed(1)
bag.boston <- randomForest(medv~., data=Boston, subset=train, mtry=13, importance=T)
bag.boston

```
note: mtry=13, means all 13 predictors will be considered for all split.
- test error
```{r}
yhat.bag <- predict(bag.boston, newdata=Boston[-train,])
plot(yhat, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
```
- tunr by ntree =25
less tree, the poor results.
```{r}
bag.boston <- randomForest(medv~., data=Boston, subset=train,mtry=13,ntree=25)

yhat.bag <- predict(bag.boston, newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
```

- let mtry = 6
```{r}
set.seed(1)
mtry= 6
rf.boston= randomForest(medv~.,data=Boston , subset=train ,
mtry=mtry, importance =TRUE)
yhat.rf = predict(rf.boston ,newdata=Boston[-train ,])
mean((yhat.rf-boston.test)^2)
```
- let mtry = sqrt(col(Boston)-1). It is the sqrt number of preditors.
it should has best performance.
Note, consideration the accuracy of sqrt, mtry should be + or - 1 of the sqrt(predicitons number)
```{r}
set.seed(1)
mtry= sqrt(ncol(Boston)-1)
rf.boston= randomForest(medv~.,data=Boston , subset=train ,
mtry=mtry, importance =TRUE)
yhat.rf = predict(rf.boston ,newdata=Boston[-train ,])
mean((yhat.rf-boston.test)^2)
```

- show the importance
```{r}
importance(rf.boston)
varImpPlot (rf.boston)

plot(sort(importance(rf.boston)))
plot(sort(varImpPlot(rf.boston)))

```

### 8.3.4 Boosting
using gbm package

distribtion ='gaussian' for regression problem.
distribution =' bernoulli' for classification problem.
n.tree and interaction.depth
```{r}
library(gbm)
set.seed(1)
boost.boston <- gbm(medv~., data=Boston[train,],
                    distribution='gaussian',
                    n.trees=5000,
                    interaction.depth=4,
                    shrinkage = 0.001)
summary(boost.boston)
```

```{r}
par(mfrow=c(1,2))
plot(boost.boston,i='rm')
plot(boost.boston,i='lstat')
```

- check the test 
for setup the shrinkage = 0.001 the value is same as textbook.
11.8.
I guess, the funciton default shrinkage is change from 0.001 to other value.
after double check textbook and software online help, the reason is 
textbook(unknow gbm version) - shrinkage = 0.001, 
gbm(2.1.8) - shrinkage = 0.1

```{r}
yhat.boost_def <- predict(boost.boston, newdata = Boston[-train,],n.trees=5000)
mean((yhat.boost_def-boston.test)^2)
```
- boost with lambda =0.2
on the textbook 0.2 could get higher result, for unknow reason, default value get it already. 
It will be check later.
Note:. below code is not reproducible as not set.seed.
for first run, it result is 11.5109.

```{r}

boost.boston=gbm(medv~.,
                 data=Boston[train ,],
                 distribution= "gaussian",
                 n.trees =5000,
                 interaction.depth =4,
                 shrinkage =0.2,
                 verbose=F)
yhat.boost2=predict(boost.boston,
                   newdata =Boston[-train ,],
                   n.trees=5000)
mean((yhat.boost2 - boston.test)^2)
```